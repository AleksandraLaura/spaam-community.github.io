# SPAAM2

## Programme

### Session Structure

Length: 2 days

All times CEST (see here for Time
Zone[conversion](https://everytimezone.com/s/e15fccf5))

#### Day 1

- _13:00_ Welcome and Introductions
- _13:30_ **Session 1** [Trash in, trash out: Optimizing and standardizing
  laboratory practices in ancient metagenomics](#session-1)
- _15:00_ Break
- _15:15_ **Session 2** [Removing the persistent trash: Challenges in genotyping
  and filtering out contaminant reads from genome alignment](#session-2).
- _17:30_ Long Break
- _18:30_ **Session 3** [Sorting the trash: Defining authentication guidelines
  for the research community](#session-3).
- _20:00_ Daily Wrap up
- _20:15_ Finish

#### Day 2

- _13:00_ Day Overview
- _13:10_ **Session 4** [Recycling the trash: Standards, reproducibility and
  open science in ancient metagenomics](#session-4).
- _15:30_ Break
- _15:45_ **Session 5** [Reuse the refuse: Applying new analytical methods
  beyond current practises](#session-5)
- _18:00_ Long Break
- _19:00_ **Session 6** [Discussion: Future steps for SPAAM](#session-6)
- _20:00_ Daily Wrap up
- _20:15_ Finish

### Abstracts

All sessions will include 'icebreaker' talks by members of the SPAAM community,
who will present on their own experiences and challenges working ancient
metagenomics to serve as starting point for discussions.

#### Session 1

**Title** Trash in, trash out: Optimizing and standardizing laboratory practices
in ancient metagenomics

**Chair** Irina Velsko

**Abstract** Optimizing laboratory practices is necessary to reduce the amount
of contaminating DNA in a sample extract, and subsequently in sequencing data.
All sequenced contaminating DNA takes up information that should be for sample
DNA, reducing the efficiency of workflows, both financially and
bioinformatically. Further, sample biomass influences the amount of "spill-over"
that is observed in sequence data, where the contents of higher biomass samples
may show up in lower biomass samples. In this session we will discuss how best
to prevent or reduce introduction of contamination to sample extraction and
library building protocols, as well as limiting sample cross-contamination.
These techniques aim to improve sequencing efficiency by reducing the amount of
sequenced contamination and increasing the amount of sequenced sample DNA.

**Icebreaker presenters** Jaelle Brealey and Zandra Fagernäs

**Icebreaker mini-abstract** _Reducing contamination: how to minimize the amount
of trash_ Contaminants can be introduced to samples during excavation,
post-excavation handling and storage and processing in the laboratory. Minor
differences between sample processing batches and cross-contamination between
samples processed together can also cause problems in downstream analyses. In
this talk we will discuss methods to reduce and control for external
contamination, batch effects and cross-contamination during laboratory
processing of ancient samples.

#### Session 2

**Title** Removing the persistent trash: Challenges in genotyping and filtering
out contaminant reads from genome alignments

**Chair** Åshild Vågene

**Abstract** Genotyping is the characterization of variants (SNPs, Indels) that
occur at the single genome level. Accurate genotyping of ancient microbial
genomes is crucial for downstream analyses, such as phylogenetic placement, SNP
effect analysis and molecular dating. Genotyping is also important for taxonomic
profiling when trying to perform strain separation. Thus, inaccuracies in
genotyping/variant calling can have an enormous impact on the interpretation of
the results.

Confounding factors related to these genomes being a) ancient, b) from
metagenomic backgrounds, can negatively impact genotyping efforts by causing
multiallelic variant calls that infringe on thresholds for calling homozygous
variants. These factors include ancient DNA damage, short reads prone to
mis-mapping in regions of genomic rearrangement and cross-mapping of
contaminating sequences derived from genetically similar organisms from the
specimens’ burial/storage environment. Approaches such as edit distance and
analysing the allele frequency of multiallelic positions are commonly used to
give an indication of the level of ‘contamination’ in an alignment. Both
molecular and computational approaches exist to mitigate the effects of
contaminant and mis-mapping reads in genotype calling.

In this session we will discuss issues and problems relating to genotyping of
ancient microbial single-genomes, with emphasis on (but not restricted to)
haploid bacterial and viral genomes. The aim of the session is not only to make
others aware of specific issues/scenarios that may arise from working with
different types of microbes, but also to have a general discussion of the best
approaches to tackle such problems.

**Icebreaker presenters** Susanna Sabin and Kun Huang

**Icebreaker mini-abstract 1** _One man's trash..._ Though consensus sequences
are required for many valuable analyses, researchers in ancient DNA and
microbial genomics in general must also acknowledge that single consensus
sequences called from NGS data rarely reflect biological reality, even in clonal
organisms. If the aDNA community is able to methodologically and culturally step
outside the phylogenetics box, it could open the door to important, time
structured evolutionary insight we lose by not considering the true variation of
target organisms within our samples. The problem is, what is true variation in
an ancient metagenomic sample?

**Icebreaker mini-abstract 2** _Building consensus alleles with awareness of
position-specific damage probability_ To reconstruct the whole genome sequence
of ancient bacteria, many recent studies use alignment-based approach by 1)
aligning sequencing reads against a single reference sequence and 2) extracting
consensus base from aligned reads. However, this strategy is artefact-prone when
consensus site is decided by bases which are positioned on the ends of most
aligned ancient reads where post-mortem damages are frequently observed.
Therefore, we introduce a possible computational solution which could ameliorate
the quality of reconstructed ancient bacterial genomes by precisely removing
nucleotide bases of ancient reads which are possibly affected by post-mortem
damage.

#### Session 3 - Sorting the trash: Defining authentication guidelines for the research community

**Title**  Sorting the trash: Defining authentication guidelines for the
research community

**Chair** Clio der Sarkissian

**Abstract** Authenticating ancient metagenomic datasets has long been
considered a nearly impossible task. Post-mortem, ancient DNA molecules
accumulate two main types of damage quantifiable in sequence data: cytosine
deamination giving rise to artifactual mutations, and depurination responsible
for high fragmentation. As a consequence, ancient DNA analyses are extremely
sensitive to contamination by exogenous DNA, which can occur anytime from
deposition to laboratory work. Ancient DNA datasets thus consist in a mixture of
DNA sequencing reads arising not only from the individuals of interest, but also
from various contaminating sources. This can potentially lead to false positive
taxon identification, inaccurate reconstructions of community compositions or
errors in genome characterisations. Validation of results is therefore
imperative to ensure scientific rigour and ethical practices. It requires the
complex task of carefully sorting ancient DNA datasets  to trash - possibly
degraded - contaminants and retain endogenous, biologically relevant
information. Furthermore, among researchers from associated fields such as
archaeology and anthropology, the challenges in authentication have
unfortunately sometimes led to the misconceptions that ancient DNA results
cannot be trusted, or to the adoption of arbitrary experimental strategies and
authentication criteria. The aim of this session will be to define what
constitutes legitimate authentication criteria as well as guidelines for their
reporting. We will then evaluate their criticality with regards to a minimal
authentication line of evidence.  We will also discuss authentication criteria
as part of the practices and processes for a responsible and ethical research
conduct. We will finally explore strategies to best communicate about ancient
DNA authentication with collaborators both within and outside the ancient
metagenomic community.

**Icebreaker presenters** Miriam Bravo López

**Icebreaker mini-abstract** The emergence of biological and historical
interpretations through ancient DNA research, in particular through ancient
pathogen genomics, can have important social, legal, and political consequences
for individuals and communities. Therefore, the discussion of the best ethical
practices within the field will allow us to promote responsible research on
ancient pathogen genomics.

#### Session 4

**Title** Recycling the trash: Standards, reproducibility and open science in
ancient metagenomics

**Chair** James Fellows Yates

**Abstract** Reproducibility in the biological sciences has become a central
discussion point of the field as a whole. Given the explosion of sophisticated
analytical techniques spanning increasingly broad interdisciplinary topics,
thorough peer-review by one or two specialists is often difficult to achieve. In
this vein, large strides have been recently made in facilitating researchers to
use the exact data (e.g. deposited in public databases from NCBI and ENA) and
software environments (bioconda, software containers) of an under-review or
previous publication.

However, central to achieving reproducibility is standardisation in the
reporting of analysis and (meta)data. When researchers speak a flexible but
clearly defined ‘common language’, this facilitates rapid and reliable
consumption of types of data, and thus assists in generating more fruitful
collaboration either through peer-review or direct contributions of different
labs to common projects.

In this session will have a wide-reaching discussion covering what obstacles and
difficulties the ancient metagenomics community faces when performing and
reporting analyses and data. We will begin to conceptualise the most important
standard analyses that should be performed in the field and what common
benchmarking and comparative datasets are required. Finally, we will discuss
what is the most important metadata currently required to ensure optimal reuse
and interpretation of the rich data we produce.

**Icebreaker presenters** Sterling Wright and Nicolás Rascovan:

**Icebreaker mini-abstract** _Improving current and future practices in ancient
microbiome analyses_ In this presentation, we will discuss the different
challenges in the bioinformatic processing and analysis of ancient microbiome
data. This includes: i) compiling reference datasets and comparative analyses,
ii) using standardized authentication tools, iii) identifying the limitations of
current methods and analysis strategies, iv) recognizing community needs and
designing more efficient methods, v) publishing studies that benchmark and
validate strategies, vi) establishing database creation and handling, and vii)
promoting open source and crowdsourcing initiatives. In particular, we will
review how all of these topics affect authentication tools implemented in
ancient microbiome studies, such as the commonly used SourceTracker. We hope
that an open discussion on these issues will serve as a guide towards towards
more robust and reproducible analysis in future research

#### Session 5 - Reuse the refuse: Applying new analytical methods beyond current practises

**Title** 

**Chair** Anna Fotakis and Alex Hübner

**Abstract** Throughout its history the field of ancient genomics has
traditionally been playing catch-up by creatively adapting the advances achieved
in modern genomics to their own types of datasets. Although this process has not
been without setbacks, it steadily pushed the field forward and allowed it to
apply the same methods as when using modern DNA. The newest technological
advancement that took over metagenomics is de-novo assembly, whereby short reads
from an environmental sample are assembled into novel microbial genomes. Its
application to sequencing data from present-day samples has led to a rush of
thousands of new metagenomic-assembled genomes (MAGs) from uncultivated
microorganisms and allowed scientists to investigate the “unknown” at large
scales for the first time. However, its application to sequencing data from
ancient samples has just started and still requires thorough evaluation and
fine-tuning of its parameters to make it suitable for this special type of
sequencing data. In this session, we want to discuss the challenges that come
along with applying de-novo assembly strategies to ancient metagenomic data and
highlight potential pitfalls in the current workflows. Furthermore, we want to
develop some guidelines on how to interpret this new type of results and
establish which conclusions can be drawn from them, in order to avoid
long-lasting mistakes that come along when adapting new methods to ancient
genomics. Finally we would like to encourage novel ways of approaching datasets
building upon previous sessions of SPAAM2 regarding public data sharing
deposition.

**Icebreaker presenters** Maxime Borry

**Icebreaker mini-abstract** TBC

#### Session 6 - Future Steps for SPAAM

**Abstract** Now that we have identified common obstacles and discussed possible
solutions for our field, what's next? In this session we will summarise
everything talked over the last two days. We can define potential common
projects and potentially set up 'working commitees' to begin work on these
topics. Finally we will gather opinions on future meet ups, e.g. a SPAAM3.